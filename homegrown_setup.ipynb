{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AbbhinavJayaraman/HCI-Grad-Course/blob/main/homegrown_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GbKnkZehINw"
   },
   "source": [
    "Since ChatGPT and existing LLMs are being annoying, we are just going to follow an existing setup that can be tweaked, making use of ollama since performance on one of our laptops running an modified Fedora Linux distrubtion was able tobe fast and responsive.\n",
    "\n",
    "Still need to figure out how to connect this with either the PRAAT scripts or myprosody:\n",
    "\n",
    "Links:\n",
    "Whisper + ollama + Bark framework: https://medium.com/@vndee.huynh/build-your-own-voice-assistant-and-run-it-locally-whisper-ollama-bark-c80e6f815cba\n",
    "\n",
    "PRAAT in python: https://github.com/YannickJadoul/Parselmouth\n",
    "myprosody: https://github.com/Shahabks/myprosody\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the following packages to install properly, you will need to make sure you on python version 3.12\n",
    "\n",
    "Install python version 3.12 (we used Homebrew), and then made a python venv with it (python3.12 -m venv sts), where sts is just a name of the venv.\n",
    "\n",
    "then you can activate the venv using source sts/bin/activate\n",
    "\n",
    "also make sure you have the portaudio library installed. For Mac, make use of the Homebrew package manager. \n",
    "\n",
    "For linux distrubitions, it's best to use the default package manager's version of portaudio, as well as the development headers (for us, that was portaudio-devel on Fedora Silverblue Linux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch\n",
    "# %pip install numpy\n",
    "# %pip install openai-whisper\n",
    "# %pip install sounddevice\n",
    "# %pip install rich\n",
    "# %pip install langchain\n",
    "# %pip install langchain_community\n",
    "# %pip install scipy\n",
    "# %pip install myprosody\n",
    "# %pip install librosa\n",
    "# %pip install gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Save the STT file locally as generic name\n",
    "# process transcription\n",
    "# copy file to archive, or just return and do that after.\n",
    "\n",
    "# get the output, can just not print also\n",
    "\n",
    "# Function to delete a file\n",
    "def delete_file(file_path):\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        print(f\"File {file_path} deleted successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: {file_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while deleting file {file_path}: {e}\")\n",
    "\n",
    "\n",
    "def copy_file (dst: str, src: str, rm_src: bool=False) :\n",
    "    try:\n",
    "        shutil.copy(src, dst)  # Preserves metadata like timestamps\n",
    "        print(f\"File copied successfully from {src} to {dst}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    if rm_src :\n",
    "        delete_file(src)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myprosody as mysp\n",
    "import io\n",
    "import sys\n",
    "\n",
    "# Redirect the print output\n",
    "def detect_sr(src: str) -> int:\n",
    "    # Create a StringIO object to capture the output\n",
    "    p=src\n",
    "    c=r\"./myprosody/myprosody\"\n",
    "\n",
    "    captured_output = io.StringIO()\n",
    "    sys.stdout = captured_output  # Redirect sys.stdout to the StringIO object\n",
    "    try:\n",
    "        # Call the function whose output you want to capture\n",
    "        mysp.myspsr(p,c)\n",
    "    finally:\n",
    "        sys.stdout = sys.__stdout__  # Restore the original sys.stdout\n",
    "\n",
    "    # Get the captured output as a string\n",
    "    output = captured_output.getvalue()\n",
    "    captured_output.close()  # Close the StringIO object\n",
    "    \n",
    "    final_syl_sec = 3.0\n",
    "    try:\n",
    "        final_syl_sec = int(output.split(\" \")[1].strip())\n",
    "    except Exception as e:\n",
    "        print(e, output)\n",
    "    \n",
    "    return final_syl_sec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the program, make sure that ollama is installed (we used Homebrew). You can then use the 'ollama serve' to start Ollama up, use 'ollama pull mistral' to pull a specific LLM (mistral, in this case). \n",
    "\n",
    "You do have to specify the name in llm input variable to the ConversationChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "7lvktKT1g0HD",
    "outputId": "9fffd0e6-2dfc-4aec-a0d4-22ea67a328a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayabbhi/Documents/HCI_grad_project/lets-talk-tempo/sts/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ollama\n\u001b[0;32m----> 8\u001b[0m stt \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtiny\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m console \u001b[38;5;241m=\u001b[39m Console()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# console = Console(log_path=False)  # Avoid recursive logging\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/HCI_grad_project/lets-talk-tempo/sts/lib/python3.12/site-packages/whisper/__init__.py:150\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found; available models = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_models()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m     )\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m    148\u001b[0m     io\u001b[38;5;241m.\u001b[39mBytesIO(checkpoint_file) \u001b[38;5;28;01mif\u001b[39;00m in_memory \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(checkpoint_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m--> 150\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m checkpoint_file\n\u001b[1;32m    153\u001b[0m dims \u001b[38;5;241m=\u001b[39m ModelDimensions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheckpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdims\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/HCI_grad_project/lets-talk-tempo/sts/lib/python3.12/site-packages/torch/serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1368\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/HCI_grad_project/lets-talk-tempo/sts/lib/python3.12/site-packages/torch/serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1851\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/Documents/HCI_grad_project/lets-talk-tempo/sts/lib/python3.12/site-packages/torch/serialization.py:1812\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1810\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1811\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1812\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/Documents/HCI_grad_project/lets-talk-tempo/sts/lib/python3.12/site-packages/torch/serialization.py:1784\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1779\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1784\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1785\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1786\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1787\u001b[0m )\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1790\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/Documents/HCI_grad_project/lets-talk-tempo/sts/lib/python3.12/site-packages/torch/serialization.py:1685\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrestore_location\u001b[39m(storage, location):\n\u001b[0;32m-> 1685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_restore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/HCI_grad_project/lets-talk-tempo/sts/lib/python3.12/site-packages/torch/serialization.py:601\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 601\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/HCI_grad_project/lets-talk-tempo/sts/lib/python3.12/site-packages/torch/serialization.py:540\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[1;32m    539\u001b[0m     device \u001b[38;5;241m=\u001b[39m _validate_device(location, backend_name)\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/HCI_grad_project/lets-talk-tempo/sts/lib/python3.12/site-packages/torch/storage.py:279\u001b[0m, in \u001b[0;36m_StorageBase.to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, device: torch\u001b[38;5;241m.\u001b[39mdevice, non_blocking: _bool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    278\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[_StorageBase, TypedStorage]:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/HCI_grad_project/lets-talk-tempo/sts/lib/python3.12/site-packages/torch/_utils.py:89\u001b[0m, in \u001b[0;36m_to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sparse\n\u001b[1;32m     87\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse storage is not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m untyped_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 89\u001b[0m \u001b[43muntyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "from rich.console import Console\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "stt = whisper.load_model(\"tiny\")\n",
    "console = Console()\n",
    "console = Console(log_path=False)  # Avoid recursive logging\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful and friendly AI assistant. You are polite and respectful. You will only give one response, in the same language as the input text.\n",
    "The conversation transcript is as follows:\n",
    "{history}\n",
    "And here is the user's follow-up: {input}\n",
    "Your response:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "chain = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"Assistant:\"),\n",
    "    llm=Ollama(model=\"llama3.2:1b\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "eQjhQ5Qmg7Vv"
   },
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import librosa\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def record_audio(stop_event, data_queue):\n",
    "    \"\"\"\n",
    "    Captures audio data from the user's microphone and adds it to a queue for further processing.\n",
    "    Args:\n",
    "        stop_event (threading.Event): An event that, when set, signals the function to stop recording.\n",
    "        data_queue (queue.Queue): A queue to which the recorded audio data will be added.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    def callback(indata, frames, time, status):\n",
    "        if status:\n",
    "            console.print(status)\n",
    "        data_queue.put(bytes(indata))\n",
    "\n",
    "    with sd.RawInputStream(\n",
    "        samplerate=44100, dtype=\"int32\", channels=1, callback=callback\n",
    "    ):\n",
    "\n",
    "        while not stop_event.is_set():\n",
    "            time.sleep(0.1)\n",
    "\n",
    "def transcribe(audio_np: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Transcribes the given audio data using the Whisper speech recognition model.\n",
    "    Args:\n",
    "        audio_np (numpy.ndarray): The audio data to be transcribed.\n",
    "    Returns:\n",
    "        str: The transcribed text.\n",
    "    \"\"\"\n",
    "    result = stt.transcribe(audio_np, fp16=True)  # Set fp16=True if using a GPU\n",
    "    text = result[\"text\"].strip()\n",
    "    return text\n",
    "\n",
    "def get_llm_response(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a response to the given text using the Llama-2 language model.\n",
    "    Args:\n",
    "        text (str): The input text to be processed.\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "    \"\"\"\n",
    "    response = chain.predict(input=text)\n",
    "    if response.startswith(\"Assistant:\"):\n",
    "        response = response[len(\"Assistant:\") :].strip()\n",
    "    return response\n",
    "\n",
    "# def play_audio(audio_file):\n",
    "#     \"\"\"\n",
    "#     Plays the given audio data using the sounddevice library.\n",
    "#     Args:\n",
    "#         sample_rate (int): The sample rate of the audio data.\n",
    "#         audio_array (numpy.ndarray): The audio data to be played.\n",
    "#     Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "\n",
    "#     audio = AudioSegment.from_mp3(audio_file)\n",
    "\n",
    "#     # Convert to numpy array (this gives us access to the raw audio data)\n",
    "#     audio_data = np.array(audio.get_array_of_samples())\n",
    "\n",
    "    \n",
    "#     # Adjust the playback speed by resampling\n",
    "#     # For example, to double the speed:\n",
    "#     sd.play(audio_data, audio.frame_rate)\n",
    "#     sd.wait()\n",
    "\n",
    "def play_audio(audio_file, user_speech_rate, cur_speech_rate_speedup) -> float: \n",
    "    audio_data, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "    # What is our rate of speech in comparison to the default?\n",
    "    speed_factor = float(user_speech_rate / 2.0) \n",
    "    new_speech_rate_speedup = ((speed_factor + cur_speech_rate_speedup) / 2.0)\n",
    "\n",
    "    # To avoid jarring speed changes, we will average the new speech rate with the old. \n",
    "\n",
    "    print(f'New speech rate speedup: {new_speech_rate_speedup}')\n",
    "    audio_stretched = librosa.effects.time_stretch(audio_data, rate=new_speech_rate_speedup)\n",
    "\n",
    "    # Play the slowed-down audio\n",
    "    # sd.play(audio_stretched, sr)\n",
    "    sd.play(audio_stretched, sr)\n",
    "    sd.wait()\n",
    "    return new_speech_rate_speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9rpd7ffg-K9"
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "from gtts import gTTS\n",
    "from queue import Queue\n",
    "import threading\n",
    "\n",
    "cur_sr_speedup = 1.0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    console.print(\"[cyan]Assistant started! Press Ctrl+C to exit.\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            console.input(\n",
    "                \"Press Enter to start recording, then press Enter again to stop.\"\n",
    "            )\n",
    "\n",
    "            data_queue = Queue()  # type: ignore[var-annotated]\n",
    "            stop_event = threading.Event()\n",
    "            recording_thread = threading.Thread(\n",
    "                target=record_audio,\n",
    "                args=(stop_event, data_queue),\n",
    "            )\n",
    "            recording_thread.start()\n",
    "\n",
    "            input()\n",
    "            stop_event.set()\n",
    "            recording_thread.join()\n",
    "\n",
    "            audio_data = b\"\".join(list(data_queue.queue))\n",
    "            audio_np = (np.frombuffer(audio_data, dtype=np.int32))\n",
    "\n",
    "            if audio_np.size > 0:\n",
    "                with console.status(\"Transcribing...\", spinner=\"earth\"):\n",
    "                    # The Audio file exists here, we can pass it along to myprosody\n",
    "                    wavfile.write('user_fr.wav', 44100, audio_np)\n",
    "                    transcr = (stt.transcribe('user_fr.wav'))['text']\n",
    "                # insert with method to extract the data speech rate method we need\n",
    "                console.print(f\"[yellow]You: {transcr}\")\n",
    "\n",
    "                with console.status(\"Extracting SR... \", spinner=\"earth\"):\n",
    "                    copy_file('./myprosody/myprosody/dataset/audioFiles/', 'user_fr.wav')\n",
    "                    user_sr = detect_sr('user_fr')\n",
    "                    # user_sr = 8\n",
    "                    print(f'User Speech Rate: {user_sr} (syllables per second)')\n",
    "\n",
    "                with console.status(\"Generating response...\", spinner=\"earth\"):\n",
    "                    response = get_llm_response(transcr)\n",
    "                    console.print(f\"[cyan]Assistant: {response}\")\n",
    "                    \n",
    "                # Generate, save, and play the audio\n",
    "                with console.status(\"Processing assistant audio..\", spinner=\"earth\"):\n",
    "                    tts = gTTS(text=response, lang='en')\n",
    "                    tts.save(\"gtts.mp3\")\n",
    "                    new_speedup = play_audio(\"gtts.mp3\", user_sr, cur_sr_speedup)\n",
    "                    cur_sr_speedup = new_speedup\n",
    "\n",
    "                with console.status(\"Cleaninp up temp files..\", spinner=\"earth\"):\n",
    "                    delete_file(\"user_fr.wav\")\n",
    "                    delete_file(\"gtts.mp3\")\n",
    "                    delete_file(\"./myprosody/myprosody/dataset/audioFiles/user_fr.wav\")\n",
    "            else:\n",
    "                console.print(\n",
    "                    \"[red]No audio recorded. Please ensure your microphone is working.\"\n",
    "                )\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        console.print(\"\\n[red]Exiting...\")\n",
    "\n",
    "    console.print(\"[blue]Session ended.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
