{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AbbhinavJayaraman/HCI-Grad-Course/blob/main/homegrown_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GbKnkZehINw"
   },
   "source": [
    "Since ChatGPT and existing LLMs are being annoying, we are just going to follow an existing setup that can be tweaked, making use of ollama since performance on one of our laptops running an modified Fedora Linux distrubtion was able tobe fast and responsive.\n",
    "\n",
    "Still need to figure out how to connect this with either the PRAAT scripts or myprosody:\n",
    "\n",
    "Links:\n",
    "Whisper + ollama + Bark framework: https://medium.com/@vndee.huynh/build-your-own-voice-assistant-and-run-it-locally-whisper-ollama-bark-c80e6f815cba\n",
    "\n",
    "PRAAT in python: https://github.com/YannickJadoul/Parselmouth\n",
    "myprosody: https://github.com/Shahabks/myprosody\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the following packages to install properly, you will need to make sure you on python version 3.12\n",
    "\n",
    "Install python version 3.12 (we used Homebrew), and then made a python venv with it (python3.12 -m venv sts), where sts is just a name of the venv.\n",
    "\n",
    "then you can activate the venv using source sts/bin/activate\n",
    "\n",
    "also make sure you have the portaudio library installed. For Mac, make use of the Homebrew package manager. \n",
    "\n",
    "For linux distrubitions, it's best to use the default package manager's version of portaudio, as well as the development headers (for us, that was portaudio-devel on Fedora Silverblue Linux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install numpy\n",
    "%pip install openai-whisper\n",
    "%pip install sounddevice\n",
    "%pip install rich\n",
    "%pip install langchain\n",
    "%pip install langchain_community\n",
    "%pip install scipy\n",
    "%pip install myprosody\n",
    "%pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Save the STT file locally as generic name\n",
    "# process transcription\n",
    "# copy file to archive, or just return and do that after.\n",
    "\n",
    "# get the output, can just not print also\n",
    "\n",
    "# Function to delete a file\n",
    "def delete_file(file_path):\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        print(f\"File {file_path} deleted successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: {file_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while deleting file {file_path}: {e}\")\n",
    "\n",
    "\n",
    "def copy_file (dst: str, src: str, rm_src: bool=False) :\n",
    "    try:\n",
    "        shutil.copy(src, dst)  # Preserves metadata like timestamps\n",
    "        print(f\"File copied successfully from {src} to {dst}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    if rm_src :\n",
    "        delete_file(src)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myprosody as mysp\n",
    "import io\n",
    "import sys\n",
    "\n",
    "# Redirect the print output\n",
    "def detect_sr(src: str) -> int:\n",
    "    # Create a StringIO object to capture the output\n",
    "    p=src\n",
    "    c=r\"/var/home/jayabbhi/Documents/HCI_grad_project/myprosody/myprosody\"\n",
    "\n",
    "    captured_output = io.StringIO()\n",
    "    sys.stdout = captured_output  # Redirect sys.stdout to the StringIO object\n",
    "    try:\n",
    "        # Call the function whose output you want to capture\n",
    "        mysp.myspsr(p,c)\n",
    "    finally:\n",
    "        sys.stdout = sys.__stdout__  # Restore the original sys.stdout\n",
    "\n",
    "    # Get the captured output as a string\n",
    "    output = captured_output.getvalue()\n",
    "    captured_output.close()  # Close the StringIO object\n",
    "    \n",
    "    final_syl_sec = 8\n",
    "    try:\n",
    "        final_syl_sec = int(output.split(\" \")[1].strip())\n",
    "    except Exception as e:\n",
    "        print(e, output)\n",
    "    \n",
    "    return final_syl_sec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the program, make sure that ollama is installed (we used Homebrew). You can then use the 'ollama serve' to start Ollama up, use 'ollama pull mistral' to pull a specific LLM (mistral, in this case). \n",
    "\n",
    "You do have to specify the name in llm input variable to the ConversationChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "7lvktKT1g0HD",
    "outputId": "9fffd0e6-2dfc-4aec-a0d4-22ea67a328a1"
   },
   "outputs": [],
   "source": [
    "\n",
    "import whisper\n",
    "from rich.console import Console\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "stt = whisper.load_model(\"tiny\")\n",
    "console = Console()\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful and friendly AI assistant. You are polite and respectful. You will only give one response, in the same language as the input text.\n",
    "The conversation transcript is as follows:\n",
    "{history}\n",
    "And here is the user's follow-up: {input}\n",
    "Your response:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "chain = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"Assistant:\"),\n",
    "    llm=Ollama(model=\"mistral\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQjhQ5Qmg7Vv"
   },
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import librosa\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def record_audio(stop_event, data_queue):\n",
    "    \"\"\"\n",
    "    Captures audio data from the user's microphone and adds it to a queue for further processing.\n",
    "    Args:\n",
    "        stop_event (threading.Event): An event that, when set, signals the function to stop recording.\n",
    "        data_queue (queue.Queue): A queue to which the recorded audio data will be added.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    def callback(indata, frames, time, status):\n",
    "        if status:\n",
    "            console.print(status)\n",
    "        data_queue.put(bytes(indata))\n",
    "\n",
    "    with sd.RawInputStream(\n",
    "        samplerate=44100, dtype=\"int32\", channels=1, callback=callback\n",
    "    ):\n",
    "\n",
    "        while not stop_event.is_set():\n",
    "            time.sleep(0.1)\n",
    "\n",
    "def transcribe(audio_np: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Transcribes the given audio data using the Whisper speech recognition model.\n",
    "    Args:\n",
    "        audio_np (numpy.ndarray): The audio data to be transcribed.\n",
    "    Returns:\n",
    "        str: The transcribed text.\n",
    "    \"\"\"\n",
    "    result = stt.transcribe(audio_np, fp16=False)  # Set fp16=True if using a GPU\n",
    "    text = result[\"text\"].strip()\n",
    "    return text\n",
    "\n",
    "def get_llm_response(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a response to the given text using the Llama-2 language model.\n",
    "    Args:\n",
    "        text (str): The input text to be processed.\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "    \"\"\"\n",
    "    response = chain.predict(input=text)\n",
    "    if response.startswith(\"Assistant:\"):\n",
    "        response = response[len(\"Assistant:\") :].strip()\n",
    "    return response\n",
    "\n",
    "# def play_audio(audio_file):\n",
    "#     \"\"\"\n",
    "#     Plays the given audio data using the sounddevice library.\n",
    "#     Args:\n",
    "#         sample_rate (int): The sample rate of the audio data.\n",
    "#         audio_array (numpy.ndarray): The audio data to be played.\n",
    "#     Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "\n",
    "#     audio = AudioSegment.from_mp3(audio_file)\n",
    "\n",
    "#     # Convert to numpy array (this gives us access to the raw audio data)\n",
    "#     audio_data = np.array(audio.get_array_of_samples())\n",
    "\n",
    "    \n",
    "#     # Adjust the playback speed by resampling\n",
    "#     # For example, to double the speed:\n",
    "#     sd.play(audio_data, audio.frame_rate)\n",
    "#     sd.wait()\n",
    "\n",
    "def play_audio(audio_file, user_speech_rate):\n",
    "    audio_data, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "    # Slow down the audio (e.g., 50% slower)\n",
    "    speed_factor = float(8.0 / user_speech_rate)\n",
    "    print(f'New speech rate: {speed_factor}')\n",
    "    audio_stretched = librosa.effects.time_stretch(audio_data, rate=speed_factor)\n",
    "\n",
    "    # Play the slowed-down audio\n",
    "    # sd.play(audio_stretched, sr)\n",
    "    sd.play(audio_stretched, sr)\n",
    "    sd.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9rpd7ffg-K9"
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "from gtts import gTTS\n",
    "from queue import Queue\n",
    "import threading\n",
    "\n",
    "sample_rate = 44100\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    console.print(\"[cyan]Assistant started! Press Ctrl+C to exit.\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            console.input(\n",
    "                \"Press Enter to start recording, then press Enter again to stop.\"\n",
    "            )\n",
    "\n",
    "            data_queue = Queue()  # type: ignore[var-annotated]\n",
    "            stop_event = threading.Event()\n",
    "            recording_thread = threading.Thread(\n",
    "                target=record_audio,\n",
    "                args=(stop_event, data_queue),\n",
    "            )\n",
    "            recording_thread.start()\n",
    "\n",
    "            input()\n",
    "            stop_event.set()\n",
    "            recording_thread.join()\n",
    "\n",
    "            audio_data = b\"\".join(list(data_queue.queue))\n",
    "            audio_np = (np.frombuffer(audio_data, dtype=np.int32))\n",
    "\n",
    "            if audio_np.size > 0:\n",
    "                with console.status(\"Transcribing...\", spinner=\"earth\"):\n",
    "                    # The Audio file exists here, we can pass it along to myprosody\n",
    "                    wavfile.write('user_fr.wav', sample_rate, audio_np)\n",
    "                    transcr = (stt.transcribe('user_fr.wav'))['text']\n",
    "                # insert with method to extract the data speech rate method we need\n",
    "                console.print(f\"[yellow]You: {transcr}\")\n",
    "\n",
    "                with console.status(\"Extracting SR... \", spinner=\"earth\"):\n",
    "                    copy_file('../myprosody/myprosody/dataset/audioFiles/', 'user_fr.wav')\n",
    "                    user_sr = detect_sr('user_fr')\n",
    "                    print(f'User Speech Rate: {user_sr} (syllables per second)')\n",
    "\n",
    "                with console.status(\"Generating response...\", spinner=\"earth\"):\n",
    "                    response = get_llm_response(transcr)\n",
    "                    console.print(f\"[cyan]Assistant: {response}\")\n",
    "                    \n",
    "                # Generate, save, and play the audio\n",
    "                with console.status(\"Processing assistant audio..\", spinner=\"earth\"):\n",
    "                    tts = gTTS(text=response, lang='en')\n",
    "                    tts.save(\"gtts.mp3\")\n",
    "                    play_audio(\"gtts.mp3\", user_sr)\n",
    "\n",
    "                with console.status(\"Cleaninp up temp files..\", spinner=\"earth\"):\n",
    "                    delete_file(\"user_fr.wav\")\n",
    "                    delete_file(\"gtts.mp3\")\n",
    "                    delete_file(\"../myprosody/myprosody/dataset/audioFiles/user_fr.wav\")\n",
    "            else:\n",
    "                console.print(\n",
    "                    \"[red]No audio recorded. Please ensure your microphone is working.\"\n",
    "                )\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        console.print(\"\\n[red]Exiting...\")\n",
    "\n",
    "    console.print(\"[blue]Session ended.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
