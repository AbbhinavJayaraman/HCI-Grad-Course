{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AbbhinavJayaraman/HCI-Grad-Course/blob/main/homegrown_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GbKnkZehINw"
   },
   "source": [
    "Since ChatGPT and existing LLMs are being annoying, we are just going to follow an existing setup that can be tweaked, making use of ollama since performance on one of our laptops running an modified Fedora Linux distrubtion was able tobe fast and responsive.\n",
    "\n",
    "Still need to figure out how to connect this with either the PRAAT scripts or myprosody:\n",
    "\n",
    "Links:\n",
    "Whisper + ollama + Bark framework: https://medium.com/@vndee.huynh/build-your-own-voice-assistant-and-run-it-locally-whisper-ollama-bark-c80e6f815cba\n",
    "\n",
    "PRAAT in python: https://github.com/YannickJadoul/Parselmouth\n",
    "myprosody: https://github.com/Shahabks/myprosody\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KdpTxq6DgvC2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/jayabbhi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, BarkModel\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\",\n",
    ")\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "class TextToSpeechService:\n",
    "    def __init__(self, device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        \"\"\"\n",
    "        Initializes the TextToSpeechService class.\n",
    "        Args:\n",
    "            device (str, optional): The device to be used for the model, either \"cuda\" if a GPU is available or \"cpu\".\n",
    "            Defaults to \"cuda\" if available, otherwise \"cpu\".\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.processor = AutoProcessor.from_pretrained(\"suno/bark-small\")\n",
    "        self.model = BarkModel.from_pretrained(\"suno/bark-small\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def synthesize(self, text: str, voice_preset: str = \"v2/en_speaker_1\"):\n",
    "        \"\"\"\n",
    "        Synthesizes audio from the given text using the specified voice preset.\n",
    "        Args:\n",
    "            text (str): The input text to be synthesized.\n",
    "            voice_preset (str, optional): The voice preset to be used for the synthesis. Defaults to \"v2/en_speaker_1\".\n",
    "        Returns:\n",
    "            tuple: A tuple containing the sample rate and the generated audio array.\n",
    "        \"\"\"\n",
    "        inputs = self.processor(text, voice_preset=voice_preset, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            audio_array = self.model.generate(**inputs, pad_token_id=10000)\n",
    "\n",
    "        audio_array = audio_array.cpu().numpy().squeeze()\n",
    "        sample_rate = self.model.generation_config.sample_rate\n",
    "        return sample_rate, audio_array\n",
    "\n",
    "    def long_form_synthesize(self, text: str, voice_preset: str = \"v2/en_speaker_1\"):\n",
    "        \"\"\"\n",
    "        Synthesizes audio from the given long-form text using the specified voice preset.\n",
    "        Args:\n",
    "            text (str): The input text to be synthesized.\n",
    "            voice_preset (str, optional): The voice preset to be used for the synthesis. Defaults to \"v2/en_speaker_1\".\n",
    "        Returns:\n",
    "            tuple: A tuple containing the sample rate and the generated audio array.\n",
    "        \"\"\"\n",
    "        pieces = []\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        silence = np.zeros(int(0.25 * self.model.generation_config.sample_rate))\n",
    "\n",
    "        for sent in sentences:\n",
    "            sample_rate, audio_array = self.synthesize(sent, voice_preset)\n",
    "            pieces += [audio_array, silence.copy()]\n",
    "\n",
    "        return self.model.generation_config.sample_rate, np.concatenate(pieces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q316uTho_Z9Z",
    "outputId": "88b9024c-8d1f-46e3-a47e-b769e6c14893"
   },
   "outputs": [],
   "source": [
    "# %pip install sounddevice\n",
    "# %pip install rich\n",
    "# %pip install langchain\n",
    "# %pip install langchain_community\n",
    "# !apt install libasound-dev portaudio19-dev libportaudio2 libportaudiocpp0\n",
    "# %pip install pyaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bUZh7AjcIQqQ",
    "outputId": "dd7148a7-76b7-4e06-9f08-0f6b08fb38bc"
   },
   "outputs": [],
   "source": [
    "# %pip install -U git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "7lvktKT1g0HD",
    "outputId": "9fffd0e6-2dfc-4aec-a0d4-22ea67a328a1"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import numpy as np\n",
    "import wave\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "from queue import Queue\n",
    "from rich.console import Console\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "console = Console()\n",
    "stt = whisper.load_model(\"base.en\")\n",
    "tts = TextToSpeechService()\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful and friendly AI assistant. You are polite, respectful, and aim to provide concise responses of less\n",
    "than 20 words.\n",
    "The conversation transcript is as follows:\n",
    "{history}\n",
    "And here is the user's follow-up: {input}\n",
    "Your response:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "chain = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"Assistant:\"),\n",
    "    llm=Ollama(model=\"mistral\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eQjhQ5Qmg7Vv"
   },
   "outputs": [],
   "source": [
    "def record_audio(stop_event, data_queue):\n",
    "    \"\"\"\n",
    "    Captures audio data from the user's microphone and adds it to a queue for further processing.\n",
    "    Args:\n",
    "        stop_event (threading.Event): An event that, when set, signals the function to stop recording.\n",
    "        data_queue (queue.Queue): A queue to which the recorded audio data will be added.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    def callback(indata, frames, time, status):\n",
    "        if status:\n",
    "            console.print(status)\n",
    "        data_queue.put(bytes(indata))\n",
    "\n",
    "    with sd.RawInputStream(\n",
    "        samplerate=16000, dtype=\"int16\", channels=1, callback=callback\n",
    "    ):\n",
    "\n",
    "        while not stop_event.is_set():\n",
    "            time.sleep(0.1)\n",
    "\n",
    "def transcribe(audio_np: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Transcribes the given audio data using the Whisper speech recognition model.\n",
    "    Args:\n",
    "        audio_np (numpy.ndarray): The audio data to be transcribed.\n",
    "    Returns:\n",
    "        str: The transcribed text.\n",
    "    \"\"\"\n",
    "    result = stt.transcribe(audio_np, fp16=False)  # Set fp16=True if using a GPU\n",
    "    text = result[\"text\"].strip()\n",
    "    return text\n",
    "\n",
    "def get_llm_response(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a response to the given text using the Llama-2 language model.\n",
    "    Args:\n",
    "        text (str): The input text to be processed.\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "    \"\"\"\n",
    "    response = chain.predict(input=text)\n",
    "    if response.startswith(\"Assistant:\"):\n",
    "        response = response[len(\"Assistant:\") :].strip()\n",
    "    return response\n",
    "\n",
    "def play_audio(sample_rate, audio_array):\n",
    "    \"\"\"\n",
    "    Plays the given audio data using the sounddevice library.\n",
    "    Args:\n",
    "        sample_rate (int): The sample rate of the audio data.\n",
    "        audio_array (numpy.ndarray): The audio data to be played.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    sd.play(audio_array, sample_rate)\n",
    "    sd.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "e9rpd7ffg-K9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Assistant started! Press Ctrl+C to exit.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mAssistant started! Press Ctrl+C to exit.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Press Enter to start recording, then press Enter again to stop.</pre>\n"
      ],
      "text/plain": [
       "Press Enter to start recording, then press Enter again to stop."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace38f2276244b38acaa576fb28aa4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">You: Hello, how is your day?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mYou: Hello, how is your day?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3b55d7cde8415780a2dfc54f476fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">Exiting...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[31mExiting\u001b[0m\u001b[31m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">Session ended.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mSession ended.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.io import wavfile\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    console.print(\"[cyan]Assistant started! Press Ctrl+C to exit.\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            console.input(\n",
    "                \"Press Enter to start recording, then press Enter again to stop.\"\n",
    "            )\n",
    "\n",
    "            data_queue = Queue()  # type: ignore[var-annotated]\n",
    "            stop_event = threading.Event()\n",
    "            recording_thread = threading.Thread(\n",
    "                target=record_audio,\n",
    "                args=(stop_event, data_queue),\n",
    "            )\n",
    "            recording_thread.start()\n",
    "\n",
    "            input()\n",
    "            stop_event.set()\n",
    "            recording_thread.join()\n",
    "\n",
    "            audio_data = b\"\".join(list(data_queue.queue))\n",
    "            audio_np = (\n",
    "                np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "            )\n",
    "\n",
    "            if audio_np.size > 0:\n",
    "                with console.status(\"Transcribing...\", spinner=\"earth\"):\n",
    "                    # The Audio file exists here, we can pass it along to myprosody\n",
    "                    wavfile.write('output3.wav', 16000, audio_np)\n",
    "                    text = transcribe(audio_np)\n",
    "                console.print(f\"[yellow]You: {text}\")\n",
    "\n",
    "                with console.status(\"Generating response...\", spinner=\"earth\"):\n",
    "                    response = get_llm_response(text)\n",
    "                    sample_rate, audio_array = tts.long_form_synthesize(response)\n",
    "\n",
    "                console.print(f\"[cyan]Assistant: {response}\")\n",
    "                play_audio(sample_rate, audio_array)\n",
    "            else:\n",
    "                console.print(\n",
    "                    \"[red]No audio recorded. Please ensure your microphone is working.\"\n",
    "                )\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        console.print(\"\\n[red]Exiting...\")\n",
    "\n",
    "    console.print(\"[blue]Session ended.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
